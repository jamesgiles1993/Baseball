{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7d9e2ecd-906a-4598-bee8-ceb569b94e67",
   "metadata": {},
   "source": [
    "# M01. Impute Inputs\n",
    "- Normalizes model inputs\n",
    "- Normalizes Steamer projections\n",
    "- Uses Steamer projections to impute model inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "99989d7f-4603-42c9-a27f-5331b84e4aa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "here\n",
      "Done\n",
      "Code was last run on: 2023-11-06\n"
     ]
    }
   ],
   "source": [
    "%run \"U1. Imports.ipynb\"\n",
    "%run \"U2. Utilities.ipynb\"\n",
    "%run \"U3. Classes.ipynb\"\n",
    "%run \"D3. Simulation Functions.ipynb\"\n",
    "\n",
    "baseball_path = r'C:\\Users\\james\\Documents\\MLB\\Database'\n",
    "\n",
    "db_path = r'C:\\Users\\james\\Documents\\MLB\\Database\\MLBDB.db'\n",
    "engine = create_engine(f'sqlite:///{db_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "252fa54f-dfa3-4540-be44-a09370aff664",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b5adf7fa-f6f7-4716-9be8-ce27f6a1831e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run \"A03. Steamer.ipynb\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff256e5d-9037-44a5-96a9-7205a3082e83",
   "metadata": {},
   "source": [
    "### Batters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e1e418d-69a3-4206-8b8b-f79df11154fc",
   "metadata": {},
   "source": [
    "##### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7c197dcd-79ee-4022-b9c5-822955cdb521",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose the last instance of each player in each game, assuming they have enough PAs\n",
    "sql_query = f'''\n",
    "WITH ranked_data AS (\n",
    "  SELECT *,\n",
    "         ROW_NUMBER() OVER (PARTITION BY gamePk, batter ORDER BY atBatIndex DESC) AS rn\n",
    "  FROM \"Dataset\"\n",
    ")\n",
    "SELECT *\n",
    "FROM ranked_data\n",
    "WHERE pa_b >= 40 AND pa_b <= 300 AND rn = 1\n",
    "'''\n",
    "\n",
    "hitters_df = pd.read_sql_query(sql_query, con=engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "145780ed-7793-458e-8057-c6baee6a3e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize the data using StandardScaler\n",
    "batter_stats_scaler = StandardScaler()\n",
    "hitters_df[batter_stats] = batter_stats_scaler.fit_transform(hitters_df[batter_stats])\n",
    "\n",
    "# Save the trained StandardScaler object\n",
    "with open(os.path.join(model_path, \"batter_stats_scaler_20231027.pkl\"), \"wb\") as file:\n",
    "    pickle.dump(batter_stats_scaler, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb70b22d-ea26-45fe-8e4f-39b364405933",
   "metadata": {},
   "source": [
    "##### Steamer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7e4df768-053a-4be7-9204-76ce745fdfe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose the last instance of each player in each game, assuming they have enough PAs\n",
    "sql_query = f'''\n",
    "  SELECT *\n",
    "  FROM \"Steamer Hitters\"\n",
    "  WHERE \"PA\" >= 40\n",
    "'''\n",
    "\n",
    "steamer_hitters_df = pd.read_sql_query(sql_query, con=engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "69a60b6f-b41c-4415-b3b2-3a4836ec3cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean\n",
    "steamer_hitters_df2 = clean_steamer_hitters(steamer_hitters_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8493f60f-a989-49b9-b043-c4cdad6f7e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize the data using StandardScaler\n",
    "batter_stats_fg_scaler = StandardScaler()\n",
    "steamer_hitters_df2[batter_stats_fg] = batter_stats_fg_scaler.fit_transform(steamer_hitters_df2[batter_stats_fg])\n",
    "\n",
    "# Save the trained StandardScaler object\n",
    "with open(os.path.join(model_path, \"batter_stats_fg_scaler_20231027.pkl\"), \"wb\") as file:\n",
    "    pickle.dump(batter_stats_fg_scaler, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a628181e-a882-478c-b176-425e6776f562",
   "metadata": {},
   "source": [
    "##### Merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a5a7bece-39ec-4ba5-9662-24252c7649b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create column steamer_date column in hitters_df equal to the highest number <= a number in this list of uniques\n",
    "steamer_dates = list(steamer_hitters_df2['date'].unique())\n",
    "\n",
    "# Define a function to find the largest number in \"steamer_dates\" less than or equal to a given \"date\"\n",
    "def find_steamer_date(date):\n",
    "    max_steamer_date = max(filter(lambda d: d <= date, steamer_dates), default=None)\n",
    "    return max_steamer_date\n",
    "\n",
    "# Apply the function to create the \"steamer_date\" column in your DataFrame\n",
    "hitters_df[\"steamer_date\"] = hitters_df[\"date\"].apply(find_steamer_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b056a781-2864-44df-9c62-11d6d4d6b268",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Steamer stats we want to keep\n",
    "batter_stats_fg_plus = ['mlbamid', 'steamerid', 'date'] + batter_stats_fg \n",
    "# Merge\n",
    "hitters_merged_df = pd.merge(hitters_df, steamer_hitters_df2[batter_stats_fg_plus], left_on=['batter', 'steamer_date'], right_on=['mlbamid', 'date'], how='inner')\n",
    "# Only keep one observation per player per game \n",
    "# Consider only keeping one observation per player per week/Steamer weekly projection\n",
    "hitters_merged_df.drop_duplicates(['gamePk', 'batter'], inplace=True, keep='last')\n",
    "# Only keep those without missing data\n",
    "hitters_merged_df = hitters_merged_df.dropna(subset=batter_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5928065-2556-4181-8989-5df4e7e55e35",
   "metadata": {},
   "source": [
    "##### Impute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "35213501-028b-4fe8-b006-f967c7fe98d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "10611/10611 [==============================] - 8s 717us/step - loss: 0.6458\n",
      "Epoch 2/20\n",
      "10611/10611 [==============================] - 8s 716us/step - loss: 0.6298\n",
      "Epoch 3/20\n",
      "10611/10611 [==============================] - 8s 777us/step - loss: 0.6269\n",
      "Epoch 4/20\n",
      "10611/10611 [==============================] - 9s 828us/step - loss: 0.6250\n",
      "Epoch 5/20\n",
      "10611/10611 [==============================] - 8s 769us/step - loss: 0.6234\n",
      "Epoch 6/20\n",
      "10611/10611 [==============================] - 8s 783us/step - loss: 0.6220\n",
      "Epoch 7/20\n",
      "10611/10611 [==============================] - 8s 711us/step - loss: 0.6210\n",
      "Epoch 8/20\n",
      "10611/10611 [==============================] - 7s 705us/step - loss: 0.6204\n",
      "Epoch 9/20\n",
      "10611/10611 [==============================] - 8s 716us/step - loss: 0.6198\n",
      "Epoch 10/20\n",
      "10611/10611 [==============================] - 8s 713us/step - loss: 0.6192\n",
      "Epoch 11/20\n",
      "10611/10611 [==============================] - 7s 698us/step - loss: 0.6187\n",
      "Epoch 12/20\n",
      "10611/10611 [==============================] - 7s 701us/step - loss: 0.6184\n",
      "Epoch 13/20\n",
      "10611/10611 [==============================] - 7s 697us/step - loss: 0.6181\n",
      "Epoch 14/20\n",
      "10611/10611 [==============================] - 7s 700us/step - loss: 0.6178\n",
      "Epoch 15/20\n",
      "10611/10611 [==============================] - 7s 705us/step - loss: 0.6176\n",
      "Epoch 16/20\n",
      "10611/10611 [==============================] - 8s 725us/step - loss: 0.6174\n",
      "Epoch 17/20\n",
      "10611/10611 [==============================] - 7s 680us/step - loss: 0.6172\n",
      "Epoch 18/20\n",
      "10611/10611 [==============================] - 7s 688us/step - loss: 0.6170\n",
      "Epoch 19/20\n",
      "10611/10611 [==============================] - 7s 679us/step - loss: 0.6170\n",
      "Epoch 20/20\n",
      "10611/10611 [==============================] - 7s 681us/step - loss: 0.6167\n"
     ]
    }
   ],
   "source": [
    "# Add hands to use in imputation\n",
    "batter_stats_fg_imp = batter_stats_fg + ['b_L', 'p_L']\n",
    "\n",
    "# Separate the features and target columns\n",
    "features = hitters_merged_df[batter_stats_fg_imp]\n",
    "target = hitters_merged_df[batter_stats]\n",
    "\n",
    "# Create and fit the model\n",
    "batter_imputations_model = keras.Sequential([\n",
    "    keras.layers.Dense(25, activation='relu', input_shape=(len(batter_stats_fg_imp),)),\n",
    "    keras.layers.Dense(25, activation='relu'),\n",
    "    keras.layers.Dense(25, activation='relu'),\n",
    "    keras.layers.Dense(len(batter_stats))  \n",
    "    ])\n",
    "\n",
    "# Compile the model\n",
    "batter_imputations_model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "\n",
    "# Train the model\n",
    "batter_imputations_model.fit(features, target, epochs=20, batch_size=32)\n",
    "\n",
    "# Pickle\n",
    "with open(os.path.join(model_path, \"batter_imputations_model_20231027.pkl\"), \"wb\") as file:\n",
    "    pickle.dump(batter_imputations_model, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c3bb1b8d-b6bb-4e4d-999b-c684c9d4a25a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10611/10611 [==============================] - 5s 438us/step\n"
     ]
    }
   ],
   "source": [
    "# Use the trained model to make predictions\n",
    "hitters_merged_df[batter_stats] = batter_imputations_model.predict(hitters_merged_df[batter_stats_fg_imp])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f0fd4a0-097d-4599-927c-78ce2f9ba4f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ce47a9ed-14d5-4631-98e6-a20a087cc3b4",
   "metadata": {},
   "source": [
    "### Pitchers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5611736-0eeb-4379-904b-3b1ad38bc613",
   "metadata": {},
   "source": [
    "##### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dcbd7fe3-9ab6-4c45-91bb-5d20a42438dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select dataset\n",
    "# Choose the last instance of each player in each game, assuming they have enough PAs\n",
    "sql_query = f'''\n",
    "WITH ranked_data AS (\n",
    "  SELECT *,\n",
    "         ROW_NUMBER() OVER (PARTITION BY gamePk, pitcher ORDER BY atBatIndex DESC) AS rn\n",
    "  FROM \"Dataset\"\n",
    ")\n",
    "SELECT *\n",
    "FROM ranked_data\n",
    "WHERE pa_p >= 40 AND pa_p <= 300 AND rn = 1\n",
    "'''\n",
    "\n",
    "pitchers_df = pd.read_sql_query(sql_query, con=engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "15ad4577-d277-472e-a9ba-89786c504656",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize the data using StandardScaler\n",
    "pitcher_stats_scaler = StandardScaler()\n",
    "pitchers_df[pitcher_stats] = pitcher_stats_scaler.fit_transform(pitchers_df[pitcher_stats])\n",
    "\n",
    "# Save the trained StandardScaler object\n",
    "with open(os.path.join(model_path, \"pitcher_stats_scaler_20231027.pkl\"), \"wb\") as file:\n",
    "    pickle.dump(pitcher_stats_scaler, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f7825ce-6237-42ed-bf67-920bb567f049",
   "metadata": {},
   "source": [
    "##### Steamer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "40764769-7db4-453a-8c3e-992c51eabe44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose the last instance of each player in each game, assuming they have enough PAs\n",
    "sql_query = f'''\n",
    "  SELECT *\n",
    "  FROM \"Steamer Pitchers\"\n",
    "  WHERE \"PA\" >= 40\n",
    "'''\n",
    "\n",
    "steamer_pitchers_df = pd.read_sql_query(sql_query, con=engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e86a9c2d-b86f-4c2b-aa68-99eca469b43b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean\n",
    "steamer_pitchers_df2 = clean_steamer_pitchers(steamer_pitchers_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "802083ad-62f5-467f-9028-fcc0046809a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "steamer_pitchers_df2.dropna(subset=pitcher_stats_fg2, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bc838279-016c-4dc6-ab32-835ebef2c33c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize the data using StandardScaler\n",
    "pitcher_stats_fg_scaler = StandardScaler()\n",
    "pitcher_stats_fg_scaled = pitcher_stats_fg_scaler.fit_transform(steamer_pitchers_df2[pitcher_stats_fg])\n",
    "pitcher_stats_fg_scaled = pd.DataFrame(pitcher_stats_fg_scaled, columns=pitcher_stats_fg)\n",
    "\n",
    "# Save the trained StandardScaler object\n",
    "with open(os.path.join(model_path, \"pitcher_stats_fg_scaler_20231027.pkl\"), \"wb\") as file:\n",
    "    pickle.dump(pitcher_stats_fg_scaler, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "260568f8-f4c5-4624-8737-f00f7235b886",
   "metadata": {},
   "source": [
    "##### Merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fb45f0de-4ea8-4ecb-aaae-6148088de959",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create column steamer_date column in hitters_df equal to the highest number <= a number in this list of uniques\n",
    "steamer_dates = list(steamer_pitchers_df2['date'].unique())\n",
    "\n",
    "# Define a function to find the largest number in \"steamer_dates\" less than or equal to a given \"date\"\n",
    "def find_steamer_date(date):\n",
    "    max_steamer_date = max(filter(lambda d: d <= date, steamer_dates), default=None)\n",
    "    return max_steamer_date\n",
    "\n",
    "# Apply the function to create the \"steamer_date\" column in your DataFrame\n",
    "pitchers_df[\"steamer_date\"] = pitchers_df[\"date\"].apply(find_steamer_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "adc859ce-02aa-43f9-96e9-51ce8e30f194",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Steamer stats we want to keep\n",
    "pitcher_stats_fg_plus = ['mlbamid', 'steamerid', 'date'] + pitcher_stats_fg2 \n",
    "# Merge\n",
    "pitchers_merged_df = pd.merge(pitchers_df, steamer_pitchers_df2[pitcher_stats_fg_plus], left_on=['pitcher', 'steamer_date'], right_on=['mlbamid', 'date'], how='inner')\n",
    "# Only keep one observation per player per game \n",
    "# Consider only keeping one observation per player per week/Steamer weekly projection\n",
    "pitchers_merged_df.drop_duplicates(['gamePk', 'pitcher'], inplace=True, keep='last')\n",
    "# Only keep those without missing data\n",
    "pitchers_merged_df = pitchers_merged_df.dropna(subset=pitcher_stats).dropna(subset=pitcher_stats_fg2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6258abfc-58ec-43e1-902f-8403c7fb2941",
   "metadata": {},
   "source": [
    "##### Impute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d3c70472-84ea-4568-a0fc-2395e4e72a65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "4991/4991 [==============================] - 4s 709us/step - loss: 0.8243\n",
      "Epoch 2/20\n",
      "4991/4991 [==============================] - 4s 716us/step - loss: 0.7824\n",
      "Epoch 3/20\n",
      "4991/4991 [==============================] - 4s 715us/step - loss: 0.7749\n",
      "Epoch 4/20\n",
      "4991/4991 [==============================] - 4s 716us/step - loss: 0.7715\n",
      "Epoch 5/20\n",
      "4991/4991 [==============================] - 4s 707us/step - loss: 0.7701\n",
      "Epoch 6/20\n",
      "4991/4991 [==============================] - 4s 709us/step - loss: 0.7690\n",
      "Epoch 7/20\n",
      "4991/4991 [==============================] - 4s 706us/step - loss: 0.7681\n",
      "Epoch 8/20\n",
      "4991/4991 [==============================] - 4s 711us/step - loss: 0.7673\n",
      "Epoch 9/20\n",
      "4991/4991 [==============================] - 4s 728us/step - loss: 0.7667\n",
      "Epoch 10/20\n",
      "4991/4991 [==============================] - 4s 710us/step - loss: 0.7662\n",
      "Epoch 11/20\n",
      "4991/4991 [==============================] - 4s 711us/step - loss: 0.7656\n",
      "Epoch 12/20\n",
      "4991/4991 [==============================] - 4s 713us/step - loss: 0.7653\n",
      "Epoch 13/20\n",
      "4991/4991 [==============================] - 4s 718us/step - loss: 0.7649\n",
      "Epoch 14/20\n",
      "4991/4991 [==============================] - 4s 710us/step - loss: 0.7643\n",
      "Epoch 15/20\n",
      "4991/4991 [==============================] - 4s 709us/step - loss: 0.7641\n",
      "Epoch 16/20\n",
      "4991/4991 [==============================] - 4s 716us/step - loss: 0.7639\n",
      "Epoch 17/20\n",
      "4991/4991 [==============================] - 4s 706us/step - loss: 0.7638\n",
      "Epoch 18/20\n",
      "4991/4991 [==============================] - 4s 718us/step - loss: 0.7634\n",
      "Epoch 19/20\n",
      "4991/4991 [==============================] - 4s 703us/step - loss: 0.7633\n",
      "Epoch 20/20\n",
      "4991/4991 [==============================] - 4s 707us/step - loss: 0.7629\n"
     ]
    }
   ],
   "source": [
    "# Add hands to use in imputation\n",
    "pitcher_stats_fg_imp = pitcher_stats_fg + ['b_L', 'p_L']\n",
    "\n",
    "# Separate the features and target columns\n",
    "features = pitchers_merged_df[pitcher_stats_fg_imp]\n",
    "target = pitchers_merged_df[pitcher_stats]\n",
    "\n",
    "# Create and fit the model\n",
    "pitcher_imputations_model = keras.Sequential([\n",
    "    keras.layers.Dense(25, activation='relu', input_shape=(len(pitcher_stats_fg_imp),)),\n",
    "    keras.layers.Dense(25, activation='relu'),\n",
    "    keras.layers.Dense(25, activation='relu'),\n",
    "    keras.layers.Dense(len(pitcher_stats))  \n",
    "    ])\n",
    "\n",
    "# Compile the model\n",
    "pitcher_imputations_model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "\n",
    "# Train the model\n",
    "pitcher_imputations_model.fit(features, target, epochs=20, batch_size=32)\n",
    "\n",
    "# Pickle\n",
    "with open(os.path.join(model_path, \"pitcher_imputations_model_20231027.pkl\"), \"wb\") as file:\n",
    "    pickle.dump(pitcher_imputations_model, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bb67163a-66e1-4dc2-a182-de7077cc04fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4991/4991 [==============================] - 2s 476us/step\n"
     ]
    }
   ],
   "source": [
    "# Use the trained model to make predictions\n",
    "pitchers_merged_df[pitcher_stats] = pitcher_imputations_model.predict(pitchers_merged_df[pitcher_stats_fg_imp])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f99cd00-827b-4614-a2bc-0a09ac9e35b6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
