{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d54e1ade-7583-4241-b152-9c3b7fee1817",
   "metadata": {},
   "source": [
    "# B01. Matchups\n",
    "- Create matchup files with batter and pitcher stats for both the home and away teams\n",
    "- Files are unimputed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "54cd67c4-d621-4682-8c04-f9dde54ab752",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run \"U1. Imports.ipynb\"\n",
    "%run \"U2. Utilities.ipynb\"\n",
    "%run \"U3. Classes.ipynb\"\n",
    "\n",
    "baseball_path = r'C:\\Users\\james\\Documents\\MLB\\Database'\n",
    "\n",
    "db_path = r'C:\\Users\\james\\Documents\\MLB\\Database\\MLBDB.db'\n",
    "engine = create_engine(f'sqlite:///{db_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b29e2fc3-97b4-42f4-aa8b-21117b06dc17",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run \"A02. MLB API.ipynb\"\n",
    "%run \"A03. Steamer.ipynb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3cf8a89-2504-414a-b82a-9c497a24b46e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f0103945-e973-4e46-91f0-4445781eddca",
   "metadata": {},
   "source": [
    "### Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49ab58b4-d3f3-44f2-9b80-6441285d72c8",
   "metadata": {},
   "source": [
    "##### Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a1ddc301-f9d8-4f5d-b5cc-b83084879699",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batters\n",
    "with open(os.path.join(model_path, \"batter_stats_scaler_20231027.pkl\"), \"rb\") as file:\n",
    "    batter_stats_scaler = pickle.load(file)\n",
    "# Pitchers\n",
    "with open(os.path.join(model_path, \"pitcher_stats_scaler_20231027.pkl\"), \"rb\") as file:\n",
    "    pitcher_stats_scaler = pickle.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b4fc02c-6304-43c0-abf8-448e73fe1b1e",
   "metadata": {},
   "source": [
    "##### FanGraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1ee01662-f36c-4942-b3e6-b6b1f46735be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batters\n",
    "with open(os.path.join(model_path, \"batter_stats_fg_scaler_20231027.pkl\"), \"rb\") as file:\n",
    "    batter_stats_fg_scaler = pickle.load(file)\n",
    "# Pitchers\n",
    "with open(os.path.join(model_path, \"pitcher_stats_fg_scaler_20231027.pkl\"), \"rb\") as file:\n",
    "    pitcher_stats_fg_scaler = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdc9168a-6094-4f18-9d69-ef1e7288303d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "02bacfc2-4519-4a59-9fd6-9c30b13fc869",
   "metadata": {},
   "source": [
    "### Clean Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b0e066b6-3d4e-4f46-b98b-3420ecc81d82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean draftable players \n",
    "def clean_draftables(draftGroupId):\n",
    "    # Read in draftables\n",
    "    sql_query = f'''\n",
    "    SELECT *\n",
    "    FROM \"Draftables {draftGroupId}\"\n",
    "    '''\n",
    "\n",
    "    draftables = pd.read_sql_query(sql_query, con=engine)\n",
    "    \n",
    "    # Replace @ sign for easier splitting\n",
    "    draftables['Game Info2'] = draftables['Game Info'].str.replace(\"@\", \" \")\n",
    "    # Extract game info\n",
    "    draftables[['Away', 'Home', 'date_slash', 'time', 'zone']] = draftables['Game Info2'].str.split(' ', 4, expand=True)\n",
    "    \n",
    "    # Convert the \"time\" column to datetime\n",
    "    draftables['datetime'] = pd.to_datetime(draftables['date_slash'] + ' ' + draftables['time'], format='%m/%d/%Y %I:%M%p')\n",
    "    # Sort the DataFrame by the \"time\" column\n",
    "    draftables = draftables.sort_values(by='datetime')\n",
    "\n",
    "    # Clean name\n",
    "    draftables['Name'] = draftables['Name'].apply(lambda x: unidecode.unidecode(x))\n",
    "\n",
    "    # Extract unique \"Game Info\" values and store them in a list\n",
    "    matchups = draftables['Game Info'].unique().tolist()\n",
    "    \n",
    "    return draftables, matchups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6fc568d7-cabf-4d41-9930-d8dbdb015849",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract game information such as gamePk\n",
    "def game_info(matchup, games, team_map):\n",
    "    # Determine home and away team\n",
    "    parts = matchup.split()\n",
    "    away, home = parts[0].split('@')\n",
    "    # Retrieve home team ID\n",
    "    home_id = team_map.loc[home, 'teamId']\n",
    "    # Determine date\n",
    "    date_slash = parts[1]  \n",
    "    date_dash = date_slash[6:] + \"-\" + date_slash[:2] + \"-\" + date_slash[3:5]\n",
    "    # Determine datetime \n",
    "    dt = date_dash + \" \" + parts[2] + \" \" + parts[3]\n",
    "    dt = parser.parse(dt, tzinfos={\"ET\": \"US/Eastern\"})\n",
    "    # Create a new datetime object representing 6:00 PM EST\n",
    "    est = pytz.timezone(\"US/Eastern\")\n",
    "    six_pm_est = est.localize(datetime.datetime(dt.year, dt.month, dt.day, 18, 0))\n",
    "    # Check if the parsed datetime is later than 6:00 PM EST\n",
    "    # If it is, and it's a doubleheader, it's the late game. Else, it's the early game.\n",
    "    if dt > six_pm_est:\n",
    "        late = True\n",
    "    else:\n",
    "        late = False\n",
    "    \n",
    "    gamePk = None\n",
    "    # Loop over all games\n",
    "    for game in games:   \n",
    "        # If date and home team match\n",
    "        if game['game_date'] == date_dash and game['home_id'] == home_id:\n",
    "            # And it's a doubleheader\n",
    "            if game['doubleheader'] == \"Y\":\n",
    "                # If it's late in the day and it's a double header, it's game two\n",
    "                if late and game['game_num'] == 2:\n",
    "                    gamePk = game['game_id']\n",
    "                # Else, it's game 1\n",
    "                elif not late and game['game_num'] == 1:\n",
    "                    gamePk = game['game_id']\n",
    "            # If it's not a double header, there will only be one.\n",
    "            elif game['doubleheader'] == \"N\":\n",
    "                gamePk = game['game_id']\n",
    "            # Identify venue id\n",
    "            venue_id = game['venue_id']     \n",
    "            # Away starter\n",
    "            away_starter = game['away_probable_pitcher']\n",
    "            # Home starter\n",
    "            home_starter = game['home_probable_pitcher']\n",
    "                \n",
    "                \n",
    "    return date_slash, gamePk, venue_id, away, home, away_starter, home_starter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "02e5457f-305a-4a3c-bead-cd072a7cd6dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create player dataframes\n",
    "def create_matchup_file(dkteam, date_slash, gamePk, venue_id, away_starter, home_starter, draftables, team_map, complete_dataset, steamer_hitters, steamer_pitchers):\n",
    "    # Retrieve Baseball Reference team abbreviation\n",
    "    team = team_map.loc[dkteam, 'BBREFTEAM']\n",
    "        \n",
    "    # Convert date to compatible format\n",
    "    date = date_slash[6:] + date_slash[:2] + date_slash[3:5]\n",
    "    \n",
    "    ### Roster\n",
    "    sql_query = f'''\n",
    "    SELECT *\n",
    "    FROM \"Roster {team} {date}\"\n",
    "    '''\n",
    "    roster = pd.read_sql_query(sql_query, con=engine)\n",
    "    \n",
    "    \n",
    "    ### Batting order\n",
    "    sql_query = f'''\n",
    "    SELECT *\n",
    "    FROM \"Batting Order {team} {gamePk}\"\n",
    "    '''\n",
    "    order = pd.read_sql_query(sql_query, con=engine)\n",
    "    # Would want to fix order here for upcoming games without order variable\n",
    "    \n",
    "    \n",
    "    ### Bullpen\n",
    "    sql_query = f'''\n",
    "    SELECT *\n",
    "    FROM \"Bullpen {team} {date}\"\n",
    "    '''   \n",
    "    bullpen = pd.read_sql_query(sql_query, con=engine)\n",
    "    bullpen.rename(columns={'Name': 'fullName'}, inplace=True)\n",
    "\n",
    "        \n",
    "    # Merge batting order onto roster\n",
    "    team_df = pd.merge(roster, order[['id', 'status', 'order']], on='id', how='left')\n",
    "    # Merge pitcher leverage onto roster\n",
    "    team_df = pd.merge(team_df, bullpen[['fullName', 'Leverage']], on='fullName', how='left')\n",
    "    # Merge draftables\n",
    "    team_df = pd.merge(team_df, draftables[['Name + ID', 'Name', 'ID', 'playerId', 'Position', 'Roster Position', 'Salary', 'AvgPointsPerGame']], left_on='fullName', right_on='Name', how='left')\n",
    "    \n",
    "    # Add weather\n",
    "    box = create_box(gamePk)\n",
    "    team_df['weather'] = box[0]\n",
    "    team_df['wind'] = box[1]\n",
    "    team_df['park'] = box[2]\n",
    "    team_df = clean_weather(team_df)\n",
    "    \n",
    "    # Add venue\n",
    "    team_df['venue_id'] = venue_id\n",
    "    \n",
    "    # Add starters\n",
    "    team_df['away_starter'] = away_starter\n",
    "    team_df['home_starter'] = home_starter\n",
    "    \n",
    "    # Assign Leverage of 1 to starting pitcher\n",
    "    team_df['Leverage'] = np.where((team_df['fullName'] == team_df['away_starter']) | (team_df['fullName'] == team_df['home_starter']), 1, team_df['Leverage'])\n",
    "    \n",
    "    # Determine batting order\n",
    "    team_df['batting_order'] = np.nan\n",
    "    for i in range(9):\n",
    "        team_df['batting_order'] = np.where(team_df['order'] == (i+1)*100, i+1, team_df['batting_order'])\n",
    "    \n",
    "    ### Batters\n",
    "    batter_df = team_df[team_df['position'] != \"Pitcher\"]\n",
    "    \n",
    "    ## Dataset\n",
    "    # Vs. LHP\n",
    "    vs_l = complete_dataset[complete_dataset['date'] < int(date)]\n",
    "    vs_l = vs_l[vs_l['pitchHand'] == \"L\"]\n",
    "    vs_l.drop_duplicates(subset='batter', keep='last', inplace=True)\n",
    "    \n",
    "    # Merge in stats\n",
    "    batter_df = pd.merge(batter_df, vs_l[['batter'] + batter_stats + ['imp_b']], left_on='id', right_on='batter', how='left')\n",
    "    \n",
    "    # Vs. RHP\n",
    "    vs_r = complete_dataset[complete_dataset['date'] < int(date)]\n",
    "    vs_r = vs_r[vs_r['pitchHand'] == \"R\"]\n",
    "    vs_r.drop_duplicates(subset='batter', keep='last', inplace=True)\n",
    "    \n",
    "    # Merge in stats\n",
    "    batter_df = pd.merge(batter_df, vs_r[['batter'] + batter_stats + ['imp_b']], left_on='id', right_on='batter', how='left', suffixes=(\"_l\", \"_r\"))\n",
    "    \n",
    "    ## Steamer \n",
    "    # Keep last observation before date (may switch to <= if I find projections are up early)\n",
    "    steamer_hitters_last_df = steamer_hitters[steamer_hitters['date'] < int(date)]\n",
    "    steamer_hitters_last_df.drop_duplicates(subset='mlbamid', keep='last', inplace=True)\n",
    "\n",
    "    # Merge\n",
    "    batter_df = pd.merge(batter_df, steamer_hitters_last_df, left_on='id', right_on='mlbamid', how='left', suffixes=(\"\", \"_fg\"))\n",
    "    \n",
    "    # Remove redundant variables\n",
    "    batter_df.drop(columns={'batter_l', 'batter_r', 'firstname', 'lastname', 'mlbamid'}, inplace=True)\n",
    "    \n",
    "    # Sort\n",
    "    batter_df.sort_values('batting_order', inplace=True)\n",
    "    \n",
    "    \n",
    "    ### Pitchers\n",
    "    pitcher_df = team_df[(team_df['position'] == \"Pitcher\") | (team_df['position'] == \"Two-Way Player\")]\n",
    "    \n",
    "    ## Dataset\n",
    "    # Vs. LHB\n",
    "    vs_l = complete_dataset[complete_dataset['date'] < int(date)]\n",
    "    vs_l = vs_l[vs_l['batSide'] == \"L\"]\n",
    "    vs_l.drop_duplicates(subset='pitcher', keep='last', inplace=True)\n",
    "    \n",
    "    # Merge in stats\n",
    "    pitcher_df = pd.merge(pitcher_df, vs_l[['pitcher'] + pitcher_stats + ['imp_p']], left_on='id', right_on='pitcher', how='left')\n",
    "    \n",
    "    # Vs. RHB\n",
    "    vs_r = complete_dataset[complete_dataset['date'] < int(date)]\n",
    "    vs_r = vs_r[vs_r['batSide'] == \"R\"]\n",
    "    vs_r.drop_duplicates(subset='pitcher', keep='last', inplace=True)\n",
    "    \n",
    "    # Merge in stats\n",
    "    pitcher_df = pd.merge(pitcher_df, vs_r[['pitcher'] + pitcher_stats + ['imp_p']], left_on='id', right_on='pitcher', how='left', suffixes=(\"_l\", \"_r\"))\n",
    "    \n",
    "    ## Steamer \n",
    "    # Keep last observation before date (may switch to <= if I find projections are up early)\n",
    "    steamer_pitchers_last_df = steamer_pitchers[steamer_pitchers['date'] < int(date)]\n",
    "    steamer_pitchers_last_df.drop_duplicates(subset='mlbamid', keep='last', inplace=True)\n",
    "\n",
    "    # Merge\n",
    "    pitcher_df = pd.merge(pitcher_df, steamer_pitchers_last_df, left_on='id', right_on='mlbamid', how='left', suffixes=(\"\", \"_fg\"))\n",
    "    \n",
    "    # Remove redundant variables\n",
    "    pitcher_df.drop(columns={'pitcher_l', 'pitcher_r', 'firstname', 'lastname', 'mlbamid'}, inplace=True)\n",
    "    \n",
    "    # Sort\n",
    "    pitcher_df.sort_values('Leverage', inplace=True)\n",
    "    \n",
    "    return batter_df, pitcher_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74f54fee-8a78-423c-bb46-a8ad2119df01",
   "metadata": {},
   "source": [
    "### Read Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b7eb81fa-17e7-4907-89b9-228bc31dfaa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in API/Statcast data\n",
    "def read_dataset():\n",
    "    # Choose the last instance of each player in each game, assuming they have enough PAs\n",
    "    sql_query = f'''\n",
    "        SELECT *\n",
    "        FROM \"Dataset\"\n",
    "        WHERE date >= 20220301\n",
    "        '''\n",
    "\n",
    "    df = pd.read_sql_query(sql_query, con=engine)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1e7d48ae-53ab-4f2a-b953-a880e86255bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in Steamer hitter data\n",
    "def read_steamer_hitters():\n",
    "    # Choose the last instance of each player in each game, assuming they have enough PAs\n",
    "    sql_query = f'''\n",
    "        SELECT *\n",
    "        FROM \"Steamer Hitters\"\n",
    "        WHERE proj_year >= 2022\n",
    "        '''\n",
    "\n",
    "    df = pd.read_sql_query(sql_query, con=engine)\n",
    "    df2 = clean_steamer_hitters(df)\n",
    "    \n",
    "    return df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "49b78e73-5ada-4a9d-8807-6efe172f4462",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in steamer pitcher data\n",
    "def read_steamer_pitchers():\n",
    "    # Choose the last instance of each player in each game, assuming they have enough PAs\n",
    "    sql_query = f'''\n",
    "        SELECT *\n",
    "        FROM \"Steamer Pitchers\"\n",
    "        WHERE proj_year >= 2022\n",
    "        '''\n",
    "\n",
    "    df = pd.read_sql_query(sql_query, con=engine)\n",
    "    df2 = clean_steamer_pitchers(df)\n",
    "    \n",
    "    return df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1a138bef-07a1-40d7-8151-b5c696380d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in team_map\n",
    "def read_team_map():\n",
    "    # Choose the last instance of each player in each game, assuming they have enough PAs\n",
    "    sql_query = f'''\n",
    "        SELECT DKTEAM, BBREFTEAM, teamId\n",
    "        FROM \"Team Map\"\n",
    "        '''\n",
    "\n",
    "    team_map = pd.read_sql_query(sql_query, con=engine)\n",
    "    team_map.set_index('DKTEAM', inplace=True)\n",
    "    \n",
    "    return team_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ed0fff99-0cb0-4b00-bb57-16e6d3816059",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in schedule to extract gamePKs based on teams and dates\n",
    "def read_schedule():\n",
    "    # 2022 games\n",
    "    games2022 = statsapi.schedule(start_date=\"03/04/2022\", end_date=\"11/15/2022\")\n",
    "    # 2023 games\n",
    "    games2023 = statsapi.schedule(start_date=\"03/04/2023\", end_date=todaysdate_slash)\n",
    "    # Add 'em together\n",
    "    games = games2022 + games2023\n",
    "    \n",
    "    return games"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb7d8226-4af2-4c53-be65-bdfdf6d62dcc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74dffc69-adc4-4b9d-b374-790d0c31b279",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3b66e15-0c52-4139-a1b1-f709f31d3d2b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "97b6c0ad-7287-473e-9a52-28e5284358fb",
   "metadata": {},
   "source": [
    "### Read in Datasets\n",
    "This should likely go in a dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ad1a2b7d-e8a7-46c0-97ba-36f230786c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in API/Statcast data\n",
    "complete_dataset = read_dataset()\n",
    "# Read in Steamer hitters\n",
    "steamer_hitters_df = read_steamer_hitters()\n",
    "# Read in Steamer pitchers\n",
    "steamer_pitchers_df = read_steamer_pitchers()\n",
    "# Read in team map\n",
    "team_map = read_team_map()\n",
    "# Read in all games since 2022\n",
    "games = read_schedule()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5908b928-7eaf-4ea3-8bd7-ec5976ce21ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "draftGroupId = 85143"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "09e08215-0c60-4ccf-a5dd-ca3533861799",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean draftables file and extract matchups\n",
    "draftables, matchups = clean_draftables(draftGroupId)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2a6b0512-fb40-4ea4-b1e5-8a4d1de36e37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DET@TOR 04/11/2023 07:07PM ET\n",
      "SD@NYM 04/11/2023 07:10PM ET\n",
      "CIN@ATL 04/11/2023 07:20PM ET\n",
      "CWS@MIN 04/11/2023 07:40PM ET\n",
      "SEA@CHC 04/11/2023 07:40PM ET\n",
      "KC@TEX 04/11/2023 08:05PM ET\n",
      "STL@COL 04/11/2023 08:40PM ET\n",
      "WAS@LAA 04/11/2023 09:38PM ET\n",
      "MIL@ARI 04/11/2023 09:40PM ET\n",
      "LAD@SF 04/11/2023 09:45PM ET\n"
     ]
    }
   ],
   "source": [
    "for matchup in matchups:\n",
    "    print(matchup)\n",
    "    # Extract info from matchup\n",
    "    date_slash, gamePk, venue_id, away, home, away_starter, home_starter = game_info(matchup, games, team_map)\n",
    "    # Away team\n",
    "    away_batter_df, away_pitcher_df = create_matchup_file(away, date_slash, gamePk, venue_id, away_starter, home_starter, draftables, team_map, complete_dataset, steamer_hitters_df, steamer_pitchers_df)\n",
    "    # Home team\n",
    "    home_batter_df, home_pitcher_df = create_matchup_file(home, date_slash, gamePk, venue_id, away_starter, home_starter, draftables, team_map, complete_dataset, steamer_hitters_df, steamer_pitchers_df)\n",
    "    \n",
    "    # Create folder, if it doesn't exist\n",
    "    try:\n",
    "        os.mkdir(os.path.join(baseball_path, \"B01. Matchups\", f'Matchups {draftGroupId}'))\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    # File name\n",
    "    matchup_file = matchup.replace('/', '').replace(':', '')\n",
    "    \n",
    "    # Write to Excel\n",
    "    away_batter_df.to_excel(os.path.join(baseball_path, \"B01. Matchups\", f'Matchups {draftGroupId}', f'{matchup_file}.xlsx'), sheet_name=\"AwayBatters\", engine='openpyxl', index=False)\n",
    "\n",
    "    with pd.ExcelWriter(os.path.join(baseball_path, \"B01. Matchups\", f'Matchups {draftGroupId}', f'{matchup_file}.xlsx'), mode='a', engine='openpyxl') as writer:  \n",
    "        home_batter_df.to_excel(writer, sheet_name='HomeBatters', index=False)\n",
    "        \n",
    "    with pd.ExcelWriter(os.path.join(baseball_path, \"B01. Matchups\", f'Matchups {draftGroupId}', f'{matchup_file}.xlsx'), mode='a', engine='openpyxl') as writer:  \n",
    "        away_pitcher_df.to_excel(writer, sheet_name='AwayPitchers', index=False)\n",
    "\n",
    "    with pd.ExcelWriter(os.path.join(baseball_path, \"B01. Matchups\", f'Matchups {draftGroupId}', f'{matchup_file}.xlsx'), mode='a', engine='openpyxl') as writer:  \n",
    "        home_pitcher_df.to_excel(writer, sheet_name='HomePitchers', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d03fde7a-bfe7-4fcc-a506-c1fde13c6a95",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "afe1cdf4-5312-4917-868c-dc4f4f5fb236",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('04/11/2023', 718620, 4705, 'CIN', 'ATL', 'Luis Cessa', 'Kyle Wright')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract info from matchup\n",
    "date_slash, gamePk, venue_id, away, home, away_starter, home_starter = game_info(matchups[2], games, team_map)\n",
    "date_slash, gamePk, venue_id, away, home, away_starter, home_starter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "18560b90-f0a1-462f-82fc-61c49505eaec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Away team\n",
    "away_batter_df, away_pitcher_df = create_matchup_file(away, date_slash, gamePk, venue_id, away_starter, home_starter, draftables, team_map, complete_dataset, steamer_hitters_df, steamer_pitchers_df)\n",
    "# Home team1\n",
    "home_batter_df, home_pitcher_df = create_matchup_file(home, date_slash, gamePk, venue_id, away_starter, home_starter, draftables, team_map, complete_dataset, steamer_hitters_df, steamer_pitchers_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "236ce693-4ac1-47d3-85a4-ee95547e5983",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'imp_b'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3621\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3620\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3621\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3622\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\_libs\\index.pyx:136\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\_libs\\index.pyx:163\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:5198\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:5206\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'imp_b'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mhome_batter_df\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mimp_b\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\frame.py:3505\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3503\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   3504\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 3505\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3506\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   3507\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3623\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3621\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[0;32m   3622\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m-> 3623\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3624\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3625\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3626\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3627\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3628\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'imp_b'"
     ]
    }
   ],
   "source": [
    "home_batter_df['imp_b']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95c10fb5-1709-4739-9885-bdaf36c29277",
   "metadata": {},
   "outputs": [],
   "source": [
    "home_batter_df[['Name', 'b1_b_r', 'b1_b_long_r']]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
